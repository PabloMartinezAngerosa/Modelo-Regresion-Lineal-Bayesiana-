---
title: "Proyecto Inferencia Bayesiana"
author: "Vanessa Alcalde, Luciano Garrido, Pablo Martinez Angerosa"
date: "27/11/2020"
header-includes: 
- \usepackage{float}
- \floatplacement{figure}{H}
output: 
  pdf_document:
 #   fig.caption: true
    latex_engine: xelatex
bibliography: bibliografia.bib
nocite: | 
  @paper_pol
  @libro
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,tidy=TRUE, message = FALSE, warning = FALSE,fig.pos = 'h')
options(xtable.comment=FALSE)
```


# 1 Introducción

En agosto de 1973, G.C. McDonald y R.C. Schwing [1] utilizaron Regresión Ridge 
para encontrar un modelo cuyas variables regresoras, compuestas 
por variables climáticas, socioeconómicas y de polución del aire, logran 
explicar la tasa de mortalidad de 60 ciudades estadounidenses en el año 1963. Como 
mencionan los autores, si bien los métodos estadísticos no necesariamente 
implican un relación de causa y efecto, bajo el supuesto de que esta relación 
existe, estos métodos proveen una herramienta para entender las contribuciones 
relativas a una variable de estudio. 

En esta investigación utilizamos el método de Bayes empírico para obtener las 
distribuciones de los coeficientes que explican la tasa de mortalidad sobre la 
misma base de datos que utilizaron G.C. McDonald y R.C. Schwing [1]. 
Para esto en una primera instancia se ajusta un modelo de regresión lineal 
múltiple, donde en el proceso de construcción se 
eliminaron algunas variables explicativas que no resultaron significativas. Al 
mismo tiempo los coeficientes $\beta_i$ que se obtienen del modelo lineal 
construido a partir de la muestra dada de datos pasan a ser los ejes 
principales de información a priori de los parámetros dándole al análisis el 
enfoque empírico bayesiano. Junto a esto se realiza un modelo de regresión 
lineal bayesiano y se comparan los resultados.

# 2 Datos

Los datos corresponden a 60 ciudades de Estados Unidos en el año 1963 y fueron 
obtenidos de la base de datos correspondientes al artículo original de G.C. 
McDonald and R.C. Schwing, "Instabilities of Regression Estimates Relating Air 
Pollution to Mortality" [1]. Cuenta con 16 variables cuantitativas agrupadas en 
las categorías climática, socioeconómica y de polución del aire. La Tabla 1 
muestra la descripción de las variables en la base de datos.

\renewcommand{\tablename}{Tabla}

```{r, results='asis'}
descrip <- utils::read.table('descripcion_var.txt', sep='\t',
                           header=TRUE,
                           row.names=NULL, stringsAsFactors=FALSE,
                           dec = ",",
                           encoding = "UTF-8",
                           check.names=FALSE)

base::print(xtable::xtable(descrip,
            caption = "Descripción de variables en la base de datos.",
            type="latex"),
            caption.placement = "bottom")

```


Las variables etiquetadas como HC, NOX y SO pertenecen a la categoría de 
polución del aire, como se muestra en la Figura 1, HC y NOX presentan una 
correlación elevada de 0.98 y por lo tanto la información que estas variables 
adhieren al modelo es simplemente de incertidumbre, por lo cual optamos por 
excluir la variable explicativa NOX.

Las variables PREC, JANT, JULT y HUMID pertenecen a la categoría de climáticas.

Las variables MORT, OVR65, POPN, EDUC, HOUS, DENS, NONW, WWDRK y POOR 
pertenecen a la categoría de variables socioeconómicas. Estas variables son 
importantes para poder medir las diferencias en los estados de salud de las 
distintas comunidades. La variable explicada MORT representa la tasa de 
mortalidad cada 100,000 habitantes. Existen correlaciones de 0.7 entre EDUC y 
WWDRK y entre NONW y POOR. Análisis de multicolinealidad realizados 
posteriormente en la búsqueda del ajuste del modelo lineal primario no dieron 
argumentos suficientes para excluir alguna de estas variables.

```{r, echo=FALSE, results='hide'}
datos <- utils::read.table('polucion.txt', sep='\t',
                           header=TRUE,
                           row.names=NULL, stringsAsFactors=FALSE,
                           dec = ",")
```


```{r, echo=FALSE, eval=FALSE}
base::round(stats::cor(datos[,1:16],use='pairwise.complete.obs'), 2) 
summary(datos)
```

\renewcommand{\figurename}{Figura}

```{r}
disp <- ggplot2::ggplot(datos, ggplot2::aes(x = HC, y = NOX)) + ggplot2::geom_point() +
  ggplot2::labs(x="HC", y="NOX", 
  title = "")
```

```{r, fig.cap ="Gráfico de dispersión de HC contra NOX.",fig.pos="H", fig.height = 3, fig.width = 3}
disp
```

En la Tabla 2 se muestra la media, el mínimo y el máximo para las 16 variables 
cuantitativas.

```{r, results='asis'}
resumen <- utils::read.table('summary.txt', sep='\t',
                           header=TRUE,
                           row.names=NULL, stringsAsFactors=FALSE,
                           dec = ",",
                           encoding = "UTF-8",
                           check.names=FALSE)

base::print(xtable::xtable(resumen,
            caption = "Resumen de algunas variables de interés.",
            type="latex"),
            caption.placement = "bottom")

```

\newpage

# 3 Métodos

Los tres modelos utilizados en esta investigación incluyen regresión lineal 
múltiple, modelo bayesiano y modelo bayesiano empírico. El proceso consistió 
primero en obtener un modelo de regresión lineal múltiple que cumpla con los supuestos 
teóricos y que esté compuesto por un conjunto de variables significativas. 
Posteriormente utilizamos la información obtenida de las estimaciones puntuales 
de los $\beta_{i}$ y el $\sigma^{2}$ como información a priori para realizar un modelo bayesiano 
empírico con las mismas variables resultantes del modelo de regresión. Para 
finalizar realizamos un modelo bayesiano utilizando como distribuciones a priori 
las default para los $\beta_{i}$ y el $\sigma^{2}$ presentes en el paquete 
*stan_glm* de *R* y comparamos el desempeño de ambos modelos bayesianos.

## 3.1 Metodología

Un modelo de regresión lineal múltiple es un modelo lineal en los parámetros 
en el cual la variable de respuesta, $Y$, es determinada por un conjunto de 
variables independientes, las variables explicativas. Se busca el hiperplano 
que mejor ajuste a los datos. 

\begin{equation}
\tag{1}
\label{mod_lineal}
Y_{i} = \boldsymbol{\beta}^{T}  \boldsymbol{x}_{i} + \varepsilon_{i}
\end{equation}

Asumiendo las hipótesis de Gauss-Markov los $\varepsilon_{i} \sim N(0,\sigma^{2})$ 
y son incorrelacionados. Los $\beta_{i}$ y las $\boldsymbol{x}_i$ son considerados 
constantes y por ende la variable explicada de la Ecuación (\ref{mod_lineal}) 
distribuye $y_{i} \sim normal(\beta^{T} x_{i}, \sigma^{2})$.
Las estimaciones puntuales de los $\beta_{i}$ que minimizan el error cuadrático 
medio del modelo se obtienen por el método de mínimos cuadrados ordinarios (MCO).

Por otro lado, los modelos bayesianos son capaces de sintetizar la información 
de la muestra y una creencia a priori, no muestral, utilizando el Teorema de Bayes. La 
creencia a priori de los parámetros que se quieren estimar se expresan a través 
de una distribución de probabilidad, llamada distribución a priori. Los 
parámetros a estimar, a diferencia del enfoque clásico, ya no son una estimación 
puntual sino que tienen un comportamiento de distribución dentro de una medida 
de probabilidad.

En la Ecuación (\ref{mod_lineal}) dentro de un enfoque bayesiano, los 
$\beta_{i}$ y $\sigma^{2}$ son variables aleatorias.

La ecuación conjunta resultante de la Ecuación (\ref{mod_lineal}) es

\begin{equation}
\tag{2}
\label{conjunta}
\begin{array}{l}
p\left(y_{1}, \ldots, y_{n} \mid x_{1}, \ldots x_{n}, \beta, \sigma^{2}\right) \\
=\prod_{i=1}^{n} p\left(y_{i} \mid \boldsymbol{x}_{i}, \boldsymbol{\beta}, \sigma^{2}\right) \\
=\left(2 \pi \sigma^{2}\right)^{-n / 2} \exp \left\{-\frac{1}{2 \sigma^{2}} \sum_{i=1}^{n}\left(y_{i}-\boldsymbol{\beta}^{T} \boldsymbol{x}_{i}\right)^{2}\right\}
\end{array}
\end{equation}

Si el vector de $\boldsymbol{\beta} \sim Normal \ multivariada(\beta_{0}, \Sigma_{0})$ 
obtenemos que la posteriori normal conjugada es


\begin{equation}
\tag{3}
\label{post}
\begin{array}{l}
p\left(\boldsymbol{\beta} \mid \boldsymbol{y}, \mathbf{X}, \sigma^{2}\right) \\
\propto p\left(\boldsymbol{y} \mid \mathbf{X}, \boldsymbol{\beta}, \sigma^{2}\right) \times p(\boldsymbol{\beta}) \\
\propto \exp \left\{-\frac{1}{2}\left(-2 \boldsymbol{\beta}^{T} \mathbf{X}^{T} \boldsymbol{y} / \sigma^{2}+\boldsymbol{\beta}^{T} \mathbf{X}^{T} \mathbf{X} \boldsymbol{\beta} / \sigma^{2}\right)-\frac{1}{2}\left(-2 \boldsymbol{\beta}^{T} \Sigma_{0}^{-1} \boldsymbol{\beta}_{0}+\boldsymbol{\beta}^{T} \Sigma_{0}^{-1} \boldsymbol{\beta}\right)\right\} \\
=\exp \left\{\boldsymbol{\beta}^{T}\left(\Sigma_{0}^{-1} \boldsymbol{\beta}_{0}+\mathbf{X}^{T} \boldsymbol{y} / \sigma^{2}\right)-\frac{1}{2} \boldsymbol{\beta}^{T}\left(\Sigma_{0}^{-1}+\mathbf{X}^{T} \mathbf{X} / \sigma^{2}\right) \boldsymbol{\beta}\right\}
\end{array}
\end{equation}

Un desafio importante de la estadística bayesiana es definir la información 
necesaria para construir la distribución a priori. Incluso algunas veces ni 
siquiera existe información previa o precisa que se pueda considerar como una 
creencia de los parámetros. 

Un posible enfoque a esta problemática es la utilización de un modelo bayesiano 
empírico, donde se utiliza los $\beta$ obtenidos por mínimos cuadrados para 
centrar las distribuciones a priori en base a estos parámetros puntuales 
estimados.

Los métodos empíricos de Bayes son procedimientos de inferencia estadística en 
los cuales la creencia a priori se construye a partir de los datos. Si bien, 
Kass y Wasserman (1995) en [2] sugieren que esta distribución no puede 
considerarse una previa real, la cantidad de información de $y$ que se utiliza no 
es de un margen considerable.

Otro posible problema de este enfoque es que no se conoce las distribuciones de 
los $\beta_{i}$, pero en esta investigación consideramos que son normales, 
centradas en los $\beta_{i}$ estimados por MCO y con desviaciones típicas basadas en 
los desvíos de estos parámetros.


## 3.2 Los modelos

A continuación en la Ecuación (\ref{modelo_final}) se muestra el modelo 
resultante por modelos lineales. Todas las variables de la base original 
fueron analizadas en modelos preliminares pero muchas resultaron no significativas por 
lo que no se incluyen en el modelo final. Las variables PREC, NONW y SO 
resultaron significativas a un 0.1% y JANT, JULT y EDUC al 5%. Se puede ver en 
el modelo final que quedan variables de las tres categorías principales en las 
que fueron organizadas en la base. 

Este modelo es significativo globalmente y logra una variablidad explicada de 
la $y_i$ expresada mediante el $R^{2}$ de un 0.8076 y un $R^{2}$ ajustado de 
un 0.7836 (el cual penaliza por la cantidad de regresores), logrando un 
buen despeño.

Se decidió sacar de la base de datos cinco observaciones que resultaron 
influyentes o atípicas luego de los análisis correspondientes y esto se 
corroboró dado que al excluirlas de la base y volver a ajustar el modelo se 
apreciaba un cambio sustancial en el $R^{2}$ y el valor de los $\beta_{i}$ 
estimados.

\begin{equation}
\tag{4}
\label{modelo_final}
MORT_i = \beta_0 + \beta_{1}PREC_{i} + \beta_{2}JANT_{i} + \beta_{3}JULT_{i} + \beta_{4}EDUC_{i} + \beta_{5}NONW_{i} + \beta_{6}SO_{i} + \varepsilon_{i}
\end{equation}


Donde $\varepsilon_{i} \sim N(0, \sigma^{2})$, $Cov(\varepsilon_{i},\varepsilon_{j})=0 \ \forall \ i \neq j$.

<!-- TODO: DEFINIR MODELO POR DEFAULT -->

El mismo modelo de la Ecuación (\ref{modelo_final}) se utiliza para el enfoque bayesiano y el bayesiano empírico. En la Tabla 3 se muestran el resumen de las medias y las desviaciones típicas de las distribuciones a priori normales utilizadas para los parámetros $\beta_i$ y $\sigma^{2}$ del modelo bayesiano empírico. Estos parámetros de las distribuciones a priori están basados en los resultados estimados por MCO del modelo de regresión lineal.

```{r, results='asis'}
prev <- utils::read.table('previa.txt', sep='\t',
                           header=TRUE,
                           row.names=NULL, stringsAsFactors=FALSE,
                           dec = ",",
                           encoding = "UTF-8",
                           check.names=FALSE)

base::print(xtable::xtable(prev,
            caption = "Parámetros de las distribuciones normales previas para el modelo bayesiano empírico.",
            type="latex"),
            caption.placement = "bottom")
```

# 4 Resultados

Los resultados obtenidos

```{r, eval=FALSE}
datos <- utils::read.table('polucion.txt', sep='\t',
                           header=TRUE,
                           row.names=NULL, stringsAsFactors=FALSE,
                           dec = ",")


datosML <- datos[-c(6, 28, 32, 37, 2),]

modelo2 <- rstanarm::stan_glm(MORT ~ PREC + JANT +
                                JULT + EDUC + NONW + SO,
                              data = datosML,
                              family = gaussian(link = "identity"),
                              prior_intercept = rstanarm::normal(1122.79104, 101.12442),
                              prior = rstanarm::normal(base::c(2.23936, -1.03784, -2.10246,
                                                              -12.41246, 3.89109, 0.30278),
                                                     base::c(0.52209, 0.48114, 0.98114,
                                                            5.36332, 0.63640, 0.06265)),
                              prior_aux = rstanarm::normal(26.19,0.5),
                              seed = 12345)

base::summary(modelo2)
rstanarm::prior_summary(modelo2)

# Estimates:
#   mean   sd     10%    50%    90%
# (Intercept) 1121.4   56.2 1051.6 1120.7 1195.0
# PREC           2.2    0.3    1.9    2.2    2.6
# JANT          -1.0    0.3   -1.4   -1.0   -0.7
# JULT          -2.1    0.6   -2.8   -2.1   -1.4
# EDUC         -12.3    3.1  -16.2  -12.4   -8.4
# NONW           3.9    0.3    3.5    3.9    4.3
# SO             0.3    0.0    0.3    0.3    0.4
# sigma         19.5    1.4   17.8   19.4   21.2


#prior_aux: normal
# Estimates:
#   mean   sd     10%    50%    90%
# (Intercept) 1123.2   67.4 1036.6 1122.2 1210.6
# PREC           2.2    0.4    1.8    2.2    2.7
# JANT          -1.0    0.3   -1.5   -1.0   -0.6
# JULT          -2.1    0.7   -3.0   -2.1   -1.2
# EDUC         -12.4    3.7  -17.1  -12.4   -7.7
# NONW           3.9    0.4    3.4    3.9    4.5
# SO             0.3    0.0    0.2    0.3    0.4
# sigma         27.2    0.8   26.3   27.0   28.3


modelo2
modelo2$coefficients
modelo2$stanfit


###Diagnostico de las Cadenas
bayesplot::mcmc_trace(modelo2)
bayesplot::mcmc_trace(modelo2,pars = "(Intercept)")
bayesplot::mcmc_trace(modelo2,pars = "PREC")
bayesplot::mcmc_trace(modelo2,pars = "JANT")
bayesplot::mcmc_trace(modelo2,pars = "JULT")
bayesplot::mcmc_trace(modelo2,pars = "EDUC")
bayesplot::mcmc_trace(modelo2,pars = "NONW")
bayesplot::mcmc_trace(modelo2,pars = "SO")
bayesplot::mcmc_trace(modelo2,pars = "sigma")

##Intervalos de credibilidad del 95%
bayesplot::mcmc_intervals(modelo2, prob = 0.95)
bayesplot::mcmc_intervals(modelo2 ,pars=c("PREC",
                                          "JANT",
                                          "JULT",
                                          "EDUC",
                                          "NONW",
                                          "SO"), prob = 0.95)

bayesplot::mcmc_intervals(modelo2 ,pars=c("SO"), prob = 0.95)


bayesplot::mcmc_hist(modelo2)#Histograma de los coeficientes
bayesplot::mcmc_hist_by_chain(modelo2)#Histograma de los coeficientes por cadena
bayesplot::mcmc_dens(modelo2)#Estimacion de densidad de los coeficientes

# Diagnostico de Modelo
##Predictivas posteriores
pred2<-rstanarm::posterior_predict(modelo2,draws = 100)
bayesplot::pp_check(modelo2, plotfun = "stat", stat = "mean")
bayesplot::pp_check(modelo2, plotfun = "stat", stat = "median")
bayesplot::pp_check(modelo2, plotfun = "stat", stat = "var")
bayesplot::pp_check(modelo2, plotfun = "stat", stat = "IQR")
bayesplot::pp_check(modelo2, plotfun = "stat", stat = "min")
bayesplot::pp_check(modelo2, plotfun = "stat", stat = "max")
bayesplot::pp_check(modelo2, plotfun = "stat", stat = "q75")

bayesplot::ppc_dens_overlay(datosML$MORT,pred2)

predict_2 <- predict(modelo2, datosML, interval="predict")

RMSEBayesianEmpiric = sqrt(mean((datosML$MORT - predict_2)^2))
```


<!-- TODO: tabla con estimaciones posteriori bayes empirico y bayes default, resultados rmse de los tres (ML, BE, B), convergencia:neff y R -->

<!--  ```{r, fig.cap ="Gráfico...",fig.pos="H"} -->
<!--  bayesplot::ppc_dens_overlay(datosML$MORT,pred2) -->
<!--  ``` -->

<!-- # TODO: Cambiar a bold lo necesario -->

$$\boldsymbol{\beta} \ \beta$$
$$\mathbf{\alpha} \ \beta$$


# 5 Discusión y conclusiones

# Referencias